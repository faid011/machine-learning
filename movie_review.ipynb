{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNETwRUA0O+NwtX3I5zRAK4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faid011/machine-learning/blob/main/movie_review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daqqiG6xtpnp",
        "outputId": "0c45050e-01d1-4b8a-fe8b-27d3491d27e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cpu\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "import json\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_PATH = \"/content/nsmc/ratings_train.txt\"\n",
        "TEST_PATH  = \"/content/nsmc/ratings_test.txt\"\n",
        "\n",
        "def preprocess_text_basic(text: str) -> str:\n",
        "    text = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", text)\n",
        "    text = re.sub(\"^ +\", \"\", text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "def preprocess_data(file_path):\n",
        "    df = pd.read_csv(file_path, sep=\"\\t\")\n",
        "    df = df.dropna(subset=[\"document\"])\n",
        "    df = df.drop_duplicates(subset=[\"document\"])\n",
        "    df[\"document\"] = df[\"document\"].apply(preprocess_text_basic)\n",
        "    df[\"document\"] = df[\"document\"].replace(\"\", np.nan)\n",
        "    df = df.dropna(subset=[\"document\"])\n",
        "    return df\n",
        "\n",
        "def build_vocab(df, min_freq=2):\n",
        "    counter = Counter()\n",
        "    for text in df[\"document\"]:\n",
        "        tokens = text.split()\n",
        "        counter.update(tokens)\n",
        "\n",
        "    # 0: pad, 1: unk\n",
        "    word2id = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "    for word, freq in counter.items():\n",
        "        if freq >= min_freq:\n",
        "            word2id[word] = len(word2id)\n",
        "    return word2id\n",
        "\n",
        "def encode_sentence(text, word2id, max_len):\n",
        "    tokens = text.split()\n",
        "    ids = [word2id.get(tok, word2id[\"<unk>\"]) for tok in tokens][:max_len]\n",
        "    if len(ids) < max_len:\n",
        "        ids += [word2id[\"<pad>\"]] * (max_len - len(ids))\n",
        "    return ids\n",
        "\n",
        "class NSMCDataset(Dataset):\n",
        "    def __init__(self, dataframe, word2id, max_len):\n",
        "        self.texts = dataframe[\"document\"].values\n",
        "        self.labels = dataframe[\"label\"].values.astype(\"int64\")\n",
        "        self.word2id = word2id\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        input_ids = encode_sentence(text, self.word2id, self.max_len)\n",
        "\n",
        "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "        return input_ids, label"
      ],
      "metadata": {
        "id": "eDOieQw6tyy4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embed_dim: int = 100,\n",
        "        hidden_dim: int = 128,\n",
        "        num_layers: int = 1,\n",
        "        num_classes: int = 2,\n",
        "        bidirectional: bool = True,\n",
        "        dropout: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "        )\n",
        "\n",
        "        self.bidirectional = bidirectional\n",
        "        fc_in_dim = hidden_dim * (2 if bidirectional else 1)\n",
        "        self.fc = nn.Linear(fc_in_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        output, (h_n, c_n) = self.lstm(emb)\n",
        "\n",
        "        if self.bidirectional:\n",
        "            h_forward = h_n[-2, :, :]\n",
        "            h_backward = h_n[-1, :, :]\n",
        "            h = torch.cat([h_forward, h_backward], dim=1)\n",
        "        else:\n",
        "            h = h_n[-1, :, :]\n",
        "\n",
        "        logits = self.fc(h)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "ckIOoedZuWj0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for input_ids, labels in dataloader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        total_correct += (preds == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    acc = total_correct / total_samples\n",
        "    return avg_loss, acc\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids, labels in dataloader:\n",
        "            input_ids = input_ids.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(input_ids)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            total_loss += loss.item() * labels.size(0)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            total_correct += (preds == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    acc = total_correct / total_samples\n",
        "    return avg_loss, acc\n",
        "\n",
        "MAX_LEN = 80\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "LR = 3e-4\n",
        "EMBED_DIM = 100\n",
        "HIDDEN_DIM = 128\n",
        "NUM_LAYERS = 1\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "print(\"train 데이터 전처리중...\")\n",
        "train_df = preprocess_data(TRAIN_PATH)\n",
        "print(\"test 데이터 전처리중...\")\n",
        "test_df = preprocess_data(TEST_PATH)\n",
        "\n",
        "print(\"vocab 생성중...\")\n",
        "word2id = build_vocab(train_df, min_freq=2)\n",
        "vocab_size = len(word2id)\n",
        "print(\"vocab size:\", vocab_size)\n",
        "\n",
        "train_dataset = NSMCDataset(train_df, word2id, MAX_LEN)\n",
        "test_dataset  = NSMCDataset(test_df,  word2id, MAX_LEN)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE)\n",
        "\n",
        "model = RNNClassifier(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=EMBED_DIM,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    num_classes=2,\n",
        "    bidirectional=BIDIRECTIONAL,\n",
        ").to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\n=== Epoch {epoch+1}/{EPOCHS} ===\")\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss, val_acc = evaluate(model, test_loader, criterion, device)\n",
        "\n",
        "    print(f\"[Train] loss: {train_loss:.4f}, acc: {train_acc*100:.2f}%\")\n",
        "    print(f\"[Test ] loss: {val_loss:.4f}, acc: {val_acc*100:.2f}%\")\n",
        "\n",
        "SAVE_DIR = \"/content/rnn_nsmc_model\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"model.pt\"))\n",
        "with open(os.path.join(SAVE_DIR, \"vocab.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(word2id, f, ensure_ascii=False)\n",
        "\n",
        "print(\"\\n저장 완료:\", SAVE_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vyb5uM9wuc_0",
        "outputId": "dce09729-1125-4302-9997-b0c8aec299bb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 데이터 전처리중...\n",
            "test 데이터 전처리중...\n",
            "vocab 생성중...\n",
            "vocab size: 67195\n",
            "\n",
            "=== Epoch 1/5 ===\n",
            "[Train] loss: 0.5827, acc: 67.10%\n",
            "[Test ] loss: 0.5493, acc: 70.35%\n",
            "\n",
            "=== Epoch 2/5 ===\n",
            "[Train] loss: 0.4729, acc: 76.07%\n",
            "[Test ] loss: 0.4693, acc: 76.12%\n",
            "\n",
            "=== Epoch 3/5 ===\n",
            "[Train] loss: 0.4090, acc: 79.99%\n",
            "[Test ] loss: 0.4488, acc: 77.50%\n",
            "\n",
            "=== Epoch 4/5 ===\n",
            "[Train] loss: 0.3646, acc: 82.65%\n",
            "[Test ] loss: 0.4496, acc: 77.64%\n",
            "\n",
            "=== Epoch 5/5 ===\n",
            "[Train] loss: 0.3263, acc: 84.68%\n",
            "[Test ] loss: 0.4562, acc: 77.77%\n",
            "\n",
            "저장 완료: /content/rnn_nsmc_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 저장된 모델 / vocab 로드 + 한 줄 예측\n",
        "\n",
        "def load_vocab_and_model(\n",
        "    model_dir,\n",
        "    embed_dim=100,\n",
        "    hidden_dim=128,\n",
        "    num_layers=1,\n",
        "    bidirectional=True,\n",
        "):\n",
        "    with open(os.path.join(model_dir, \"vocab.json\"), \"r\", encoding=\"utf-8\") as f:\n",
        "        word2id = json.load(f)\n",
        "    vocab_size = len(word2id)\n",
        "\n",
        "    model = RNNClassifier(\n",
        "        vocab_size=vocab_size,\n",
        "        embed_dim=embed_dim,\n",
        "        hidden_dim=hidden_dim,\n",
        "        num_layers=num_layers,\n",
        "        num_classes=2,\n",
        "        bidirectional=bidirectional,\n",
        "    )\n",
        "    state = torch.load(os.path.join(model_dir, \"model.pt\"), map_location=device)\n",
        "    model.load_state_dict(state)\n",
        "    return model, word2id\n",
        "\n",
        "def predict_one(text, model, word2id, max_len=80):\n",
        "    model.eval()\n",
        "    text_clean = preprocess_text_basic(text)\n",
        "    ids = encode_sentence(text_clean, word2id, max_len)\n",
        "    input_ids = torch.tensor([ids], dtype=torch.long).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        pred = torch.argmax(probs, dim=1).item()\n",
        "    return pred, probs[0].cpu().numpy()\n",
        "\n",
        "MODEL_DIR = \"/content/rnn_nsmc_model\"\n",
        "model_loaded, vocab_loaded = load_vocab_and_model(MODEL_DIR)\n",
        "model_loaded.to(device)\n",
        "\n",
        "while True:\n",
        "    text = input(\"리뷰 입력 (q 입력 시 종료): \").strip()\n",
        "    if text.lower() == \"q\":\n",
        "        break\n",
        "    if not text:\n",
        "        print(\"공백입니다.\\n\")\n",
        "        continue\n",
        "\n",
        "    pred, probs = predict_one(text, model_loaded, vocab_loaded, max_len=MAX_LEN)\n",
        "    label = \"긍정\" if pred == 1 else \"부정\"\n",
        "    print(f\"리뷰: {text}\")\n",
        "    print(f\"예측: {label} (부정={probs[0]:.3f}, 긍정={probs[1]:.3f})\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V352jt3k5PWU",
        "outputId": "dce50735-a102-41ba-ea41-ebe9ca7b5669"
      },
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "리뷰 입력 (q 입력 시 종료):  리얼 미국판 홍길동 ㅇㅈ\n",
            "리뷰: 리얼 미국판 홍길동 ㅇㅈ\n",
            "예측: 부정 (부정=0.729, 긍정=0.271)\n",
            "\n",
            "리뷰 입력 (q 입력 시 종료):  호스맨 기다렸다고ㅠㅠ 꼭 보세요.. 시리즈 팬들도 충분히 만족할만함\n",
            "리뷰: 호스맨 기다렸다고ㅠㅠ 꼭 보세요.. 시리즈 팬들도 충분히 만족할만함\n",
            "예측: 긍정 (부정=0.095, 긍정=0.905)\n",
            "\n",
            "리뷰 입력 (q 입력 시 종료):  이런 영화 기다려왔다... 전작보다 더 재밌어진 느낌..? 시간 가는줄 몰랐음\n",
            "리뷰: 이런 영화 기다려왔다... 전작보다 더 재밌어진 느낌..? 시간 가는줄 몰랐음\n",
            "예측: 긍정 (부정=0.214, 긍정=0.786)\n",
            "\n",
            "리뷰 입력 (q 입력 시 종료):  구관이 명관이라더니 예전에 재밌게봤던 시리즈가 나와서 반가워서 봤는데 역시 존잼탱이네요\n",
            "리뷰: 구관이 명관이라더니 예전에 재밌게봤던 시리즈가 나와서 반가워서 봤는데 역시 존잼탱이네요\n",
            "예측: 긍정 (부정=0.038, 긍정=0.962)\n",
            "\n",
            "리뷰 입력 (q 입력 시 종료):  이거 재밌다고 추천하는사람들은 억빠들인거임? 나만당할수 없다고 악질인거임?솔직히 나우유씨미 호스맨 볼라고 보는건데 호스맨 활약 전혀없고 왠 갑자기 뭔 꼬맹이하나가 나 설계자임 ㅇㅇ 하면서 깝치는건데 솔직히 스토리 너무 산으로 가는 느낌이고 그저 다음화 발사대용 0.5화 정도에 미공개판에 들어갈만한 내용들인데 길게 풀어놓은 느낌인데 범인 잡힌거 마무리도 ㅈㄴ 어이없이 갑자기 범인만 끌려가고 뭘 말하고싶은지 1도 모르겠음그냥 나 설계자임 ㅇㅇ 개쩔지? ㅇㅇ 하다가 끝나고 우리 할아범 그설계자놈때매 괜히죽고 솔직히 체인소맨 레제 2번 보는게 더 재밌음\n",
            "리뷰: 이거 재밌다고 추천하는사람들은 억빠들인거임? 나만당할수 없다고 악질인거임?솔직히 나우유씨미 호스맨 볼라고 보는건데 호스맨 활약 전혀없고 왠 갑자기 뭔 꼬맹이하나가 나 설계자임 ㅇㅇ 하면서 깝치는건데 솔직히 스토리 너무 산으로 가는 느낌이고 그저 다음화 발사대용 0.5화 정도에 미공개판에 들어갈만한 내용들인데 길게 풀어놓은 느낌인데 범인 잡힌거 마무리도 ㅈㄴ 어이없이 갑자기 범인만 끌려가고 뭘 말하고싶은지 1도 모르겠음그냥 나 설계자임 ㅇㅇ 개쩔지? ㅇㅇ 하다가 끝나고 우리 할아범 그설계자놈때매 괜히죽고 솔직히 체인소맨 레제 2번 보는게 더 재밌음\n",
            "예측: 부정 (부정=0.992, 긍정=0.008)\n",
            "\n",
            "리뷰 입력 (q 입력 시 종료): q\n"
          ]
        }
      ]
    }
  ]
}